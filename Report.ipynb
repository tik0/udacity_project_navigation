{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation Project\n",
    "\n",
    "---\n",
    "\n",
    "## Project Description\n",
    "\n",
    "This document is the report of the first Udacity project for reinforcment learning. \n",
    "\n",
    "For this project, I have train an agent to navigate and collect yellow bananas in a large, square world.\n",
    "\n",
    "<img src=\"images/banana.gif\">\n",
    "\n",
    "A reward of +1 is provided for collecting a yellow banana, and a reward of -1 is provided for collecting a blue banana. Thus, the goal of the agent is to collect as many yellow bananas as possible while avoiding blue bananas.\n",
    "\n",
    "The state space has 37 dimensions and contains the agent's velocity, along with ray-based perception of objects around the agent's forward direction. Given this information, the agent has to learn how to best select actions. Four discrete actions are available, corresponding to:\n",
    "\n",
    "* 0 - move forward.\n",
    "* 1 - move backward.\n",
    "* 2 - turn left.\n",
    "* 3 - turn right.\n",
    "\n",
    "The task is episodic, and in order to solve the environment, the agent must get an average score of +13 over 100 consecutive episodes.\n",
    "\n",
    "## Solution Approach\n",
    "\n",
    "The solution follows the vanilla approach by [the original DQN paper](https://arxiv.org/abs/1312.5602).\n",
    "But instead of having convolutional layers to derive the state from raw pixel data, we can directly access the crisp current state.\n",
    "Therefore, the whole network architecture simplifies, such that I only need two deep layers with [ReLU activation](https://arxiv.org/pdf/1706.08098.pdf) for the Q-Network.\n",
    "For further investigation, I evaluate the size of the hidden layers to find the best estimate following a fan-in approach for deeper layers.\n",
    "\n",
    "The hyperparameter for training the Q-Network are as follows:\n",
    "\n",
    "* DQN Learning Parameters\n",
    " * $n_{\\text{episodes}}$ = 2000\n",
    " * $\\epsilon_{\\text{decay}}$ = 0.995\n",
    " * $\\epsilon_{\\text{start}}$ = 1.0\n",
    " * $\\epsilon_{\\text{end}}$ = 0.01\n",
    " * Replay buffer size = 10e5\n",
    " * Minibatch size = 64\n",
    " * $\\Gamma$ (aka discount factor) = 0.99\n",
    " * $\\tau$ (aka soft update of target parameters) = 1e-3\n",
    " * Learning rate = 5e-4\n",
    " * Update Interval between local and target network = 4\n",
    "* DQN Network Architecture\n",
    " * Layer-Conections: Input -> ReLU Layer 1 -> RelU Layer 2 -> Output\n",
    " * Layer-Dimensionality [ReLU Layer 1, ReLU Layer 2]: [8,8], [16,8], [16,16], [32,8], [32,16], [32,32], [64,8], [64,16], [64,32], [64,64], [128,8], [128,16], [128,32], [128,64], [128,128]\n",
    "\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "### 1. Preliminars for the Program Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [ 1.          0.          0.          0.          0.84408134  0.          0.\n",
      "  1.          0.          0.0748472   0.          1.          0.          0.\n",
      "  0.25755     1.          0.          0.          0.          0.74177343\n",
      "  0.          1.          0.          0.          0.25854847  0.          0.\n",
      "  1.          0.          0.09355672  0.          1.          0.          0.\n",
      "  0.31969345  0.          0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from functions.dqn_agent import Agent\n",
    "from functions.rl import DQN\n",
    "from functions.helper import env_info, scores_stat\n",
    "\n",
    "# Start the environment\n",
    "env = UnityEnvironment(file_name=\"/data/Banana_Linux_NoVis/Banana.x86_64\")\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "state_size, action_size = env_info(env, brain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training the agent using a 2-Layer DQN with varying Layer-Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current config: _fc1-8_fc2-8_e-2000_decay-0.995\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 2000\n",
    "eps_decay = 0.995\n",
    "fc_units_power_min = 3\n",
    "fc_units_power_max = 6\n",
    "scores_fc_eval = []\n",
    "for fc_1_units_power in np.arange(fc_units_power_min, fc_units_power_max+1):\n",
    "    for fc_2_units_power in np.arange(fc_units_power_min, fc_1_units_power+1):\n",
    "        # Allocate the agent\n",
    "        agent = Agent(state_size=state_size,\n",
    "                      action_size=action_size,\n",
    "                      seed=0,\n",
    "                      fc1_units=np.int(2**fc_1_units_power),\n",
    "                      fc2_units=np.int(2**fc_2_units_power))\n",
    "        # Define a config string\n",
    "        filename =  str(\"_fc1-\" + str(agent.fc1_units) +\n",
    "                    \"_fc2-\" + str(agent.fc2_units) +\n",
    "                    \"_e-\" + str(n_episodes) +\n",
    "                    \"_decay-\" + str(eps_decay))\n",
    "        print(\"Current config: \" + filename)\n",
    "        # Check if the scores are arcived, otherwise train it\n",
    "        if not Path(\"eval/dict\" + filename).is_file():\n",
    "            # Train the agent\n",
    "            dqn = DQN(agent = agent, env = env, brain = brain,\n",
    "                      eps_decay = eps_decay,\n",
    "                      verbose = False,\n",
    "                      n_episodes = n_episodes,\n",
    "                      max_t = 1000)\n",
    "            # Archive the values\n",
    "            scores = dqn.train(weights_location = \"weights/network\" + filename + \".pth\")\n",
    "            scores_fc_eval.append({\"fc1_units\": agent.fc1_units,\n",
    "                                   \"fc2_units\": agent.fc2_units,\n",
    "                                   \"epsilodes\": dqn.n_episodes,\n",
    "                                   \"decay\": dqn.eps_decay,\n",
    "                                   \"scores\": scores})\n",
    "            with open(\"eval/dict\" + filename, 'wb') as fp:\n",
    "                pickle.dump(scores_fc_eval[-1], fp)\n",
    "        else:\n",
    "            scores_fc_eval.append(pickle.load( open( \"eval/dict\" + filename, \"rb\" ) ))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Three subplots sharing both x/y axes\n",
    "plot_num = fc_units_power_max - fc_units_power_min + 1\n",
    "f, axs = plt.subplots(plot_num, plot_num, figsize=(20,10), sharex=True, sharey=True)\n",
    "cnt = 0\n",
    "threashold = 13\n",
    "eval_threashold_idx = np.zeros((plot_num,plot_num))\n",
    "eval_max_score = np.zeros((plot_num,plot_num))\n",
    "for idx in np.arange(0,plot_num):\n",
    "    for idy in np.arange(0,idx+1):\n",
    "        ax = axs[idy, plot_num - idx - 1]\n",
    "        if idy == 0:\n",
    "            ax.set_title(str(scores_fc_eval[cnt]['fc1_units']))\n",
    "        if idx == plot_num - 1:\n",
    "            ax.set_ylabel(str(scores_fc_eval[cnt]['fc2_units']))\n",
    "        scores = scores_fc_eval[cnt]['scores']\n",
    "        scores_avg, scores_std = scores_stat(scores, 100)\n",
    "        eval_max_score[idy, idx] = np.nanmax(scores_avg)\n",
    "        eval_threashold_idx[idy, idx] = np.nanargmax(scores_avg > threashold)\n",
    "        ax.plot(np.arange(len(scores)), scores, linestyle='-', color='cornflowerblue', linewidth=1)\n",
    "        ax.plot(np.arange(len(scores)), scores_avg, linestyle='-', color='red', linewidth=2)\n",
    "        ax.plot([0, len(scores)-1], [threashold, threashold], linestyle='--', color='black', linewidth=2)\n",
    "        cnt = cnt + 1\n",
    "f.subplots_adjust(hspace=0)\n",
    "f.subplots_adjust(wspace=0)\n",
    "f.suptitle(\"Comparision of layer dimensionality: FC1 (x-axis) vs. FC2 (y-axis)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corresponding index, where the average score exceeds 13 for the first time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_threashold_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corresponding maximum score achived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Conclusion\n",
    "\n",
    "The evaluation shows, that "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
